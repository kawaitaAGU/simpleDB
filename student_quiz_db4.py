# app.py  â€”  å­¦ç”ŸæŒ‡å°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆå …ç‰¢ç‰ˆï¼‰
import io
import re
from datetime import datetime

import pandas as pd
import streamlit as st

# ===== åŸºæœ¬è¨­å®š =====
st.set_page_config(page_title="ğŸ“˜ å­¦ç”ŸæŒ‡å°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", layout="wide")
st.title("ğŸ” å­¦ç”ŸæŒ‡å°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """BOM/ç©ºç™½/æ”¹è¡Œã‚’é™¤å»ã—ã€ã‚ˆãã‚ã‚‹åˆ¥åã‚’æ­£è¦åã¸å¯„ã›ã‚‹"""
    def _clean(s):
        s = str(s).replace("\ufeff", "")          # BOM
        return re.sub(r"[\u3000 \t\r\n]+", "", s) # å…¨åŠè§’ç©ºç™½ã¨æ”¹è¡Œ
    df = df.copy()
    df.columns = [_clean(c) for c in df.columns]

    alias = {
        "å•é¡Œæ–‡":  ["è¨­å•", "å•é¡Œ", "æœ¬æ–‡"],
        "é¸æŠè‚¢1": ["é¸æŠè‚¢ï¼¡","é¸æŠè‚¢a","A","ï½"],
        "é¸æŠè‚¢2": ["é¸æŠè‚¢ï¼¢","é¸æŠè‚¢b","B","ï½‚"],
        "é¸æŠè‚¢3": ["é¸æŠè‚¢ï¼£","é¸æŠè‚¢c","C","ï½ƒ"],
        "é¸æŠè‚¢4": ["é¸æŠè‚¢ï¼¤","é¸æŠè‚¢d","D","ï½„"],
        "é¸æŠè‚¢5": ["é¸æŠè‚¢ï¼¥","é¸æŠè‚¢e","E","ï½…"],
        "æ­£è§£":    ["è§£ç­”","ç­”ãˆ","ans","answer"],
        "ç§‘ç›®åˆ†é¡": ["åˆ†é¡","ç§‘ç›®","ã‚«ãƒ†ã‚´ãƒª","ã‚«ãƒ†ã‚´ãƒªãƒ¼"],
    }
    existing = set(df.columns)
    for canon, cands in alias.items():
        if canon in existing:
            continue
        for c in cands:
            if c in existing:
                df.rename(columns={c: canon}, inplace=True)
                existing.add(canon)
                break
    return df

def safe_get(row: pd.Series | dict, keys, default=""):
    """Series/è¾æ›¸ã‹ã‚‰ã€å®‰å…¨ã«åˆ—å€¤ã‚’å–å¾—ï¼ˆåˆ¥åãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰"""
    if isinstance(row, pd.Series):
        row = row.to_dict()
    for k in keys:
        v = row.get(k, None)
        if v is not None:
            s = str(v).strip()
            if s != "":
                return s
    # äºˆå‚™ï¼šåˆ—åã®ç©ºç™½/BOMã‚’é™¤å»ã—ã¦ã€Œå•é¡Œæ–‡ã€ã£ã½ã„ã‚‚ã®ã‚’æ‹¾ã†
    for k, v in row.items():
        kk = re.sub(r"\s", "", str(k).replace("\ufeff", ""))
        if kk.endswith("å•é¡Œæ–‡") and v is not None and str(v).strip() != "":
            return str(v).strip()
    return default

def ensure_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    """å‡ºåŠ›CSVã«å¿…é ˆåˆ—ãŒç„¡ã‘ã‚Œã°ç©ºåˆ—ã‚’è¶³ã™"""
    need = ["å•é¡Œæ–‡","é¸æŠè‚¢1","é¸æŠè‚¢2","é¸æŠè‚¢3","é¸æŠè‚¢4","é¸æŠè‚¢5","æ­£è§£","ç§‘ç›®åˆ†é¡"]
    out = df.copy()
    for c in need:
        if c not in out.columns:
            out[c] = ""
    return out

def build_text_block(row: pd.Series | dict) -> str:
    """1å•é¡Œã¶ã‚“ã®ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢"""
    q = safe_get(row, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"]) or "ï¼ˆå•é¡Œæ–‡ãªã—ï¼‰"
    lines = [f"å•é¡Œæ–‡: {q}"]
    for i in range(1, 6):
        val = safe_get(row, [f"é¸æŠè‚¢{i}"])
        if val:
            lines.append(f"é¸æŠè‚¢{i}: {val}")
    ans = safe_get(row, ["æ­£è§£","è§£ç­”","ç­”ãˆ"])
    cat = safe_get(row, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"])
    lines.append(f"æ­£è§£: {ans}")
    lines.append(f"åˆ†é¡: {cat}")
    lines.append("-" * 40)
    return "\n".join(lines)

# ===== ãƒ‡ãƒ¼ã‚¿èª­è¾¼ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ or æ—¢å®šãƒ‘ã‚¹ï¼‰=====
with st.sidebar:
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    up = st.file_uploader("CSV ã‚’é¸æŠï¼ˆUTF-8 æ¨å¥¨ï¼‰", type=["csv"])
    default_path = st.text_input("ãƒ­ãƒ¼ã‚«ãƒ«æ—¢å®šãƒ‘ã‚¹ï¼ˆä»»æ„ï¼‰", value="")
    load_btn = st.button("èª­ã¿è¾¼ã‚€")

@st.cache_data(show_spinner=False)
def load_df(file_like, path_hint: str):
    if file_like is not None:
        df = pd.read_csv(file_like, dtype=str, encoding="utf-8-sig")
    elif path_hint:
        df = pd.read_csv(path_hint, dtype=str, encoding="utf-8-sig")
    else:
        raise FileNotFoundError("CSV ãŒæœªæŒ‡å®šã§ã™ã€‚å·¦ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‹ãƒ‘ã‚¹å…¥åŠ›ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")
    df = normalize_columns(df.fillna(""))
    return df

if load_btn:
    try:
        df = load_df(up, default_path)
        st.success(f"èª­ã¿è¾¼ã¿æˆåŠŸï¼š{len(df)} è¡Œ")
        st.session_state.df = df
    except Exception as e:
        st.error(f"èª­ã¿è¾¼ã¿å¤±æ•—ï¼š{e}")

if "df" not in st.session_state:
    st.info("å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ CSV ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
    st.stop()

df = st.session_state.df

# ===== æ¤œç´¢ =====
st.subheader("ğŸ” æ¤œç´¢")
search = st.text_input("å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ãƒ»åˆ†é¡ã§æ¤œç´¢ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§ ANDï¼‰", value="")
def contains_all(hay: str, terms: list[str]) -> bool:
    hay = (hay or "").lower()
    return all(t.lower() in hay for t in terms)

terms = [t for t in search.split() if t.strip()]
# æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆæˆ
def row_text(r: pd.Series) -> str:
    parts = [
        safe_get(r, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"]),
        *[safe_get(r, [f"é¸æŠè‚¢{i}"]) for i in range(1,6)],
        safe_get(r, ["æ­£è§£","è§£ç­”","ç­”ãˆ"]),
        safe_get(r, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"]),
    ]
    return " ".join([p for p in parts if p])

if terms:
    mask = df.apply(lambda r: contains_all(row_text(r), terms), axis=1)
    filtered_df = df[mask].reset_index(drop=True)
else:
    filtered_df = df.copy().reset_index(drop=True)

st.success(f"{len(filtered_df)} ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸ")

# ===== ãƒ¬ã‚³ãƒ¼ãƒ‰é–²è¦§ =====
if len(filtered_df) == 0:
    st.stop()

col_idx, _ = st.columns([1,3])
with col_idx:
    rec_no = st.number_input("è¡¨ç¤ºã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ç•ªå·:", min_value=0, max_value=max(0, len(filtered_df)-1), value=0, step=1)

record = filtered_df.iloc[rec_no]

st.markdown("### ğŸ§ª å•é¡Œæ–‡")
st.markdown(f"**{safe_get(record, ['å•é¡Œæ–‡','è¨­å•','å•é¡Œ','æœ¬æ–‡']) or 'ï¼ˆå•é¡Œæ–‡ãªã—ï¼‰'}**")

st.markdown("### âœ… é¸æŠè‚¢")
for i in range(1, 6):
    v = safe_get(record, [f"é¸æŠè‚¢{i}"])
    if v:
        st.write(f"- é¸æŠè‚¢{i}: {v}")

st.markdown("### ğŸ æ­£è§£ãƒ»åˆ†é¡")
st.write(f"**æ­£è§£**: {safe_get(record, ['æ­£è§£','è§£ç­”','ç­”ãˆ'])}")
st.write(f"**åˆ†é¡**: {safe_get(record, ['ç§‘ç›®åˆ†é¡','åˆ†é¡','ç§‘ç›®'])}")

# ===== ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ï¼ˆä»»æ„ãƒ¡ãƒ¢ã€‚ä¿å­˜å…ˆã¯å®Ÿè£…ã—ã¦ã„ã¾ã›ã‚“ï¼‰=====
st.text_area("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¨˜éŒ²", "")

# ===== ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====
today = datetime.now().strftime("%m%d")
filename_base = f"{(search or 'results')}_{today}"

# .txt ã¾ã¨ã‚å‡ºåŠ›
txt_buffer = io.StringIO()
for _, row in filtered_df.iterrows():
    txt_buffer.write(build_text_block(row) + "\n")

st.download_button(
    label="ğŸ“¥ ãƒ’ãƒƒãƒˆçµæœã‚’ .txt ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=txt_buffer.getvalue(),
    file_name=f"{filename_base}.txt",
    mime="text/plain"
)

# .csv å‡ºåŠ›ï¼ˆå¿…é ˆåˆ—ã‚’ãã‚ãˆã‚‹ï¼‰
csv_buffer = io.StringIO()
ensure_output_columns(filtered_df).to_csv(csv_buffer, index=False, encoding="utf-8-sig")
st.download_button(
    label="ğŸ“¥ ãƒ’ãƒƒãƒˆçµæœã‚’ .csv ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_buffer.getvalue(),
    file_name=f"{filename_base}.csv",
    mime="text/csv"
)

# ===== ãƒ‡ãƒãƒƒã‚°è£œåŠ©ï¼ˆå¿…è¦ãªã‚‰é–‹ãï¼‰=====
with st.expander("ğŸ”§ ç¾åœ¨ã®åˆ—åï¼ˆæ­£è¦åŒ–å¾Œï¼‰ã‚’è¦‹ã‚‹"):
    st.write(list(filtered_df.columns))
